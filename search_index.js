var documenterSearchIndex = {"docs":
[{"location":"advanced_boosting_models/advanced_boosting_model/#Advanced-Boosting-Model","page":"AdvancedBoostingModel","title":"Advanced Boosting Model","text":"","category":"section"},{"location":"advanced_boosting_models/advanced_boosting_model/","page":"AdvancedBoostingModel","title":"AdvancedBoostingModel","text":"AdvancedBoostingModel","category":"page"},{"location":"advanced_boosting_models/advanced_boosting_model/#AdvancedBoosting.AdvancedBoostingModel","page":"AdvancedBoostingModel","title":"AdvancedBoosting.AdvancedBoostingModel","text":"Abstract type that specifies the behavior of all 'advanced boosting models'.\n\nThese models are meant to consist of multiple standard boosting models (RootBoostingModel) and combine them into a single, non-trivial and combined model.\n\nEach model needs to be associated with a respective prediction function (m::AdvancedBoostingModel{T})(X::Matrix{T}) where {T<:AbstractFloat} and the init_intercepts! and build_trees! functions.\n\nThe former initializes the alpha_0 intercepts of each root model, whereas the latter fits the decision trees of the subsequent models. The fit! function then provides a single interface for any AdvancedBoostinModel by subsequently calling both init_intercepts! and build_trees!.\n\n\n\n\n\n","category":"type"},{"location":"root_boosting_model/#Root-Boosting-Model","page":"Root Boosting Model","title":"Root Boosting Model","text":"","category":"section"},{"location":"root_boosting_model/","page":"Root Boosting Model","title":"Root Boosting Model","text":"The underlying Gradient Boosting model that can be stacked to create new, more sophisticated models. ","category":"page"},{"location":"root_boosting_model/","page":"Root Boosting Model","title":"Root Boosting Model","text":"You typically don't want to work with these low-level models directly but embed them into higher level models.","category":"page"},{"location":"root_boosting_model/","page":"Root Boosting Model","title":"Root Boosting Model","text":"RootBoostingModel\nis_trained","category":"page"},{"location":"root_boosting_model/#AdvancedBoosting.RootBoostingModel","page":"Root Boosting Model","title":"AdvancedBoosting.RootBoostingModel","text":"Single boosting model whose inputs, outputs and parameters are of type T<:AbstractFloat.\n\nCan be constructed either via\n\nRootBoostingModel{T}(max_depth::Int64, n_trees::Int64)::RootBoostingModel{T} where T<:AbstractFloat\n\nor\n\nRootBoostingModel(max_depth::Int64, n_trees::Int64)::RootBoostingModel{Float64}\n\nwhere max_depth defines the maximum depth of each tree (typically, max_depth=1 suffices in a Gradient Boosting model) and n_trees defines the number of trees to include in the model, i.e. the number refinement iterations\n\nOnce a is_trained(RootBoostingModel{T})==true, i.e. the model has been fitted to some data, we can give it an input matrix X::Matrix{T} to produce some predictions:\n\nmodel = RootBoostingModel(1,100)\n...(training the model here)\nX = randn(5,10) #5 datapoints with 10 features each\npredictions = model(X)\n\nIt is also possible to use an input vector X::Vector{T}. In that case, X will be treated as a single-row matrix, i.e. an input matrix with a single data point.\n\nmodel = RootBoostingModel(1,100)\n...(training the model here)\nX = randn(10) # a single datapoint with 10 features\npredictions = model(X)\n\nNotice that the RootBoostingModel deliberately has no API to be trained directly. Rather, a higher level model typically employs several RootBoostingModels and the training happens via the higher level model APIs.\n\nAs a helper function, it is also possible to make predictions from a vector of RootBoostingModels as follows:\n\nmodels = [RootBoostingModel(1,100), RootBoostingModel(1,100)]\n...(training the models here)\nX = randn(10) # a single datapoint with 10 features\npredictions = models(X) #returns a vector of prediction vectors\n\n\n\n\n\n","category":"type"},{"location":"root_boosting_model/#AdvancedBoosting.is_trained","page":"Root Boosting Model","title":"AdvancedBoosting.is_trained","text":"is_trained(m::RootBoostingModel)::Bool\n\nCheck whether a boosting model has already been fitted/trained.\n\n\n\n\n\n","category":"function"},{"location":"advanced_boosting_models/distributional_boosting_model/","page":"Distributional Boosting","title":"Distributional Boosting","text":"Models the conditional distribution ","category":"page"},{"location":"advanced_boosting_models/distributional_boosting_model/","page":"Distributional Boosting","title":"Distributional Boosting","text":"p(mathbfymathbfx)=p(mathbfytheta(mathbfx))","category":"page"},{"location":"advanced_boosting_models/distributional_boosting_model/","page":"Distributional Boosting","title":"Distributional Boosting","text":"where","category":"page"},{"location":"advanced_boosting_models/distributional_boosting_model/","page":"Distributional Boosting","title":"Distributional Boosting","text":"theta(mathbfx)=left(g_1(f_1(mathbfx))g_m(f_m(mathbfx))right)^T","category":"page"},{"location":"advanced_boosting_models/distributional_boosting_model/","page":"Distributional Boosting","title":"Distributional Boosting","text":"i.e., the parameters of the conditional distribution are expressed by Gradient Boosting models f_1f_m and respective link-functions g_1g_m.","category":"page"},{"location":"advanced_boosting_models/distributional_boosting_model/","page":"Distributional Boosting","title":"Distributional Boosting","text":"DistributionalBoostingModel","category":"page"},{"location":"advanced_boosting_models/distributional_boosting_model/#AdvancedBoosting.DistributionalBoostingModel","page":"Distributional Boosting","title":"AdvancedBoosting.DistributionalBoostingModel","text":"DistributionalBoostingModel{T<:AbstractFloat, D<:Distributions.Distribution}(\n \tdist::Type{D},\n \tboosters::Vector{RootBoostingModel{T}},\n \ttransform::ParameterizableTransform\n)\n\ndist can be any Distributions.Distribution, the output of transform circ boosters then needs to provide valid parameters to that distribution.\n\nExample:\n\nusing AdvancedBoosting\n\nimport Distributions.Normal\n\nmodel = DistributionalBoostingModel(\n    Normal,\n    [RootBoostingModel(2,5),RootBoostingModel(2,5)],\n    MultiTransform([IdentityTransform([1]), SoftplusTransform([2])])\n)\n\nThe respective probabilistic model is then\n\np(ymathbfx)=mathcalN(yf_1(mathbfx)s(f_2(mathbfx))\n\nwhere f_1f_2 are individual Gradient Boosting models and s is the softplus transform:\n\ntextsoftplus(x)=log left(exp(x)+1right)\n\n\n\n\n\n","category":"type"},{"location":"advanced_boosting_models/varying_coefficient_boosting_model/","page":"-","title":"-","text":"VaryingCoefficientBoostingModel","category":"page"},{"location":"advanced_boosting_models/varying_coefficient_boosting_model/#AdvancedBoosting.VaryingCoefficientBoostingModel","page":"-","title":"AdvancedBoosting.VaryingCoefficientBoostingModel","text":"VaryingCoefficientBoostingModel{T<:AbstractFloat}(\n\tboosters::Vector{RootBoostingModel{T}},\n\ttransform::VaryingCoefficientTransform\n)\n\nExample:\n\nusing AdvancedBoosting\n\nusing Random\n\nmodel = VaryingCoefficientModel(\n\t[RootBoostingModel(1,5), RootBoostingModel(1,5)],\n        VaryingCoefficientTransform([1,2])\n)\n\nRandom.seed!(123)\nX = randn(100,2)\ny = sin.(X[:,1]) .+ cos(X[:,2])\n\nfit!(model, X, y)\n\nCurrently, this model is trained via the MSE-criterion. Other criteria can easily be added\n\n\n\n\n\n","category":"type"},{"location":"#AdvancedBoosting.jl","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"","category":"section"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"Experimental package for various Gradient Boosting models.","category":"page"},{"location":"#Some-examples","page":"AdvancedBoosting.jl","title":"Some examples","text":"","category":"section"},{"location":"#Gaussian-conditional-distribution","page":"AdvancedBoosting.jl","title":"Gaussian conditional distribution","text":"","category":"section"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"Here, we model the conditional mean and standard deivation as Gradient Boosting models, i.e.","category":"page"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"p(ymathbfx)=mathcalN(yf_1(mathbfx)s(f_2(mathbfx)))","category":"page"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"where f_1f_2 are individual Gradient Boosting models and s is the softplus function","category":"page"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"textsoftplus(x)=logleft(exp(x)+1right)","category":"page"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"using AdvancedBoosting, Random, Plots\nimport Distributions.Normal, Distributions.mean, Distributions.std\n\nRandom.seed!(321);\n\nX = rand(100,1) .* 6 .- 3;\ny = sin.(X) .+ randn(100,1) .* (0.25 .* abs.(sin.(X)) .+ 0.1);\n\n\nmodel = DistributionalBoostingModel(\n    Normal, #conditional distribution shoud be normal\n    [RootBoostingModel(1,3), RootBoostingModel(1,3)], #both conditional mean and standard deviation are modelled by GradientBoosting\n    MultiTransform(\n        [IdentityTransform([1]), #mean model output stay as is\n         SoftplusTransform([2])  #stddev model output is mapped to the positive, non-zero reals\n    ]) \n);\n\nfit!(model, X, y[:])\n\nlines = collect(-3:0.1:3)[:,:];\npred_dists = model(lines);\n\np1 = plot();\n\nplot!(p1\n    lines[:], sin.(lines[:]),\n    ribbon = 2 .* (0.25 .* sin.(lines[:]).^2 .+ 0.1),\n    label=\"Ground truth distribution\",\n    title=\"Gaussian Distribution with sin(x) mean + variance\", \n    titlefontsize=10,\n    fmt=:png,\n    lw=2\n);\nplot!(p1, \n    lines[:],mean.(pred_dists),\n    ribbon = 2 .* std.(pred_dists),\n    label=\"Gradient Boosting estimate\", lw=2\n);\nscatter!(p1,\n    X[:],y[:],\n    markersize = 0.25,\n    label = \"Data (n=100)\"\n);","category":"page"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"(Image: )","category":"page"},{"location":"#Gaussian-conditional-distribution,-censored-at-y0","page":"AdvancedBoosting.jl","title":"Gaussian conditional distribution, censored at y=0","text":"","category":"section"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"Same as above, but now we have the Gaussian distribution censored at y=0, i.e.","category":"page"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"p(ymathbfx)=Phi(0f_1(mathbfx)s(f_2(mathbfx)))cdotmathbbI(y=0) + (1-Phi(0f_1(mathbfx)s(f_2(mathbfx))))cdotmathcalN(yf_1(mathbfx)s(f_2(mathbfx)))cdotmathbbI(y0)","category":"page"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"import Distributions.Normal, Distributions.censored\nRandom.seed!(321)\n\nX = rand(200,1) .* 6 .- 3\nf(x) = censored(Normal(sin( 2 * x), 0.25*abs(sin(x))+0.1), lower=0.0)\ny = rand.(f.(X))\n\n#define custom distribution to match our type definition\nimport Distributions.ContinuousUnivariateDistribution, Distributions.logpdf, Distributions.mean, Distributions.quantile\n\nstruct ZeroCensoredNormal <: ContinuousUnivariateDistribution\n    mu\n    sigma\nend\n\nlogpdf(m::ZeroCensoredNormal, y) = logpdf(censored(Normal(m.mu, m.sigma), lower=0.0), y)\nmean(m::ZeroCensoredNormal) = mean(censored(Normal(m.mu, m.sigma), lower=0.0))\nquantile(m::ZeroCensoredNormal, p) = quantile(censored(Normal(m.mu, m.sigma), lower=0.0),p)\n\n\nmodel = DistributionalBoostingModel(\n    ZeroCensoredNormal,\n    [RootBoostingModel(1,5),RootBoostingModel(1,5)],\n    MultiTransform([IdentityTransform([1]), SoftplusTransform([2])])\n)\n\nfit!(model, X, y[:])\n\nlines = collect(-3:0.1:3)[:,:]\npred_dists = model(lines)\nmean_pred = mean.(pred_dists)\nribbon_pred = (mean_pred .- quantile.(pred_dists,0.05), quantile.(pred_dists,0.95) .- mean_pred)\n\nline_dists = f.(lines)\nmean_line = mean.(line_dists)\nribbon_line = (mean_line .- quantile.(line_dists,0.05), quantile.(line_dists,0.95) .- mean_line)\n\n\np1 = plot()\n\nplot!(p1,lines[:], mean_line, ribbon = ribbon_line, label=\"Ground truth distribution\", title=\"Gaussian Distribution with harmonic mean + variance, censored at 0.0\", \n    titlefontsize=10, fmt=:png, lw=2)\nplot!(p1, lines[:], mean_pred, ribbon = ribbon_pred, label=\"Gradient Boosting estimate\", lw=2)\nscatter!(p1, X[:],y[:], markersize = 0.25, label = \"Data (n=200)\")","category":"page"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"(Image: )","category":"page"},{"location":"#Varying-Coefficient-Boosting","page":"AdvancedBoosting.jl","title":"Varying Coefficient Boosting","text":"","category":"section"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"Model the coefficients of a linear model as gradient boosted, varying coefficients:","category":"page"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"y=alpha + f_1(mathbfx)cdot mathbfx_(1) + cdots + f_M(mathbfx)cdot mathbfx_(M)","category":"page"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"where mathbfxinmathbbR^M and f_1f_M are individual Gradient Boosting models. ","category":"page"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"using AdvancedBoosting, Random, Plots\nimport Distributions.Normal, Distributions.mean \n\nRandom.seed!(321);\n\nX = rand(100,1) .* 6 .- 3\nf(x) = Normal(0.5*x^2, 0.25)\ny = rand.(f.(X))\n\nmodel = VaryingCoefficientBoostingModel(\n    [RootBoostingModel(1,5)],\n    VaryingCoefficientTransform()\n);\n\nfit!(model, X, y[:])\n\nlines = collect(-3:0.1:3)[:,:];\npredictions = model(lines);\n\np1 = plot();\n\nplot!(p1, lines[:], mean.(f.(lines[:])),\n      label=\"Ground truth function\",\n      title=\"Varying Coefficient Boosting for a square function\", \n      titlefontsize=10,\n      fmt=:png,\n      lw=2,\n      legend=:top);\nplot!(p1, lines[:], predictions,\n      label=\"Gradient Boosting estimate\",\n      lw=2);\nscatter!(p1, X[:], y[:],\n         markersize=0.25,\n         label=\"Data (n=100)\");","category":"page"},{"location":"","page":"AdvancedBoosting.jl","title":"AdvancedBoosting.jl","text":"(Image: )","category":"page"},{"location":"parameterizable_transform/#Parameterizable-Transformations","page":"Parameterizable Transformations","title":"Parameterizable Transformations","text":"","category":"section"},{"location":"parameterizable_transform/","page":"Parameterizable Transformations","title":"Parameterizable Transformations","text":"Transformations that can contain parameters, e.g. linear transformations or neural networks. The respective transformation also needs to be differentiable with respect to both input and parameters in order to properly optimize the boosting model and additional parameters.","category":"page"},{"location":"parameterizable_transform/","page":"Parameterizable Transformations","title":"Parameterizable Transformations","text":"All transformations should be applicable using both the boosting output and the actual input.","category":"page"},{"location":"parameterizable_transform/","page":"Parameterizable Transformations","title":"Parameterizable Transformations","text":"I.e. let f(cdot) denote the function corresponding to a Gradient Boosting model, mathbfx the vector of inputs and g a ParameterizableTransform. Then, g should be applicable as","category":"page"},{"location":"parameterizable_transform/","page":"Parameterizable Transformations","title":"Parameterizable Transformations","text":"g(f(mathbfx)mathbfxtheta)","category":"page"},{"location":"parameterizable_transform/","page":"Parameterizable Transformations","title":"Parameterizable Transformations","text":"Where theta denotes a respective parameter vector of the transformation itself.","category":"page"},{"location":"parameterizable_transform/","page":"Parameterizable Transformations","title":"Parameterizable Transformations","text":"This increases flexibility and allows to, for example, build varying coefficient models where the varying coefficients are modelled by individual gradient boosting models, f_1f_n:","category":"page"},{"location":"parameterizable_transform/","page":"Parameterizable Transformations","title":"Parameterizable Transformations","text":"g(f_1(mathbfx)f_n(mathbfx)mathbfxtheta)=alpha_0 + f_1(mathbfx)cdot x_1 + cdots + f_n(mathbfx)cdot x_n","category":"page"},{"location":"parameterizable_transform/","page":"Parameterizable Transformations","title":"Parameterizable Transformations","text":"where thetaequiv alpha.","category":"page"},{"location":"parameterizable_transform/","page":"Parameterizable Transformations","title":"Parameterizable Transformations","text":"ParameterizableTransform\nIdentityTransform\nSoftplusTransform\nSigmoidTransform\nMultiTransform\nVaryingCoefficientTransform\nComposedTransform\nSumTransform","category":"page"},{"location":"parameterizable_transform/#AdvancedBoosting.ParameterizableTransform","page":"Parameterizable Transformations","title":"AdvancedBoosting.ParameterizableTransform","text":"The abstract supertype corresponding to parameterizable transformations.\n\n\n\n\n\n","category":"type"},{"location":"parameterizable_transform/#AdvancedBoosting.IdentityTransform","page":"Parameterizable Transformations","title":"AdvancedBoosting.IdentityTransform","text":"Applies the identity transformation,\n\ng(f(mathbfx) mathbfx)=f(mathbfx)_i_1i_2i_m\n\nwhere i_1i_2i_m denotes the indices defined in target_idx\n\nusing AdvancedBoosting\ntransform = IdentityTransform([1])\ntransform([2.], zeros(3))\n\n# output\n\n1-element Vector{Float64}:\n 2.0\n\nusing AdvancedBoosting\ntransform = IdentityTransform([1])\ntransform([2.])\n\n# output\n\n1-element Vector{Float64}:\n 2.0\n\n\n\n\n\n","category":"type"},{"location":"parameterizable_transform/#AdvancedBoosting.SoftplusTransform","page":"Parameterizable Transformations","title":"AdvancedBoosting.SoftplusTransform","text":"Applies the softplus transform,\n\ng(f(mathbfx)mathbfx)=log(exp(f(mathbfx)_i_1i_2i_m)+1)\n\nwhere i_1i_2i_m denotes the elements defined in target_idx.\n\nusing AdvancedBoosting\ntransform = SoftplusTransform([1])\ntransform([0.], zeros(3))\n\n# output\n\n1-element Vector{Float64}:\n 0.6931471805599453\n\nusing AdvancedBoosting\ntransform = SoftplusTransform([1])\ntransform([0.])\n\n# output\n\n1-element Vector{Float64}:\n 0.6931471805599453\n\n\n\n\n\n","category":"type"},{"location":"parameterizable_transform/#AdvancedBoosting.SigmoidTransform","page":"Parameterizable Transformations","title":"AdvancedBoosting.SigmoidTransform","text":"Applies the sigmoid transform,\n\ng(f(mathbfx)mathbfx)=frac11+exp(-f(mathbfx)_i_1i_2i_m)\n\nwhere i_1i_2i_m denotes the elements defined in target_idx.\n\nusing AdvancedBoosting\ntransform = SigmoidTransform([1])\ntransform([0.], zeros(3))\n\n# output\n\n1-element Vector{Float64}:\n 0.5\n\nusing AdvancedBoosting\ntransform = SigmoidTransform([1])\ntransform([0.])\n\n# output\n\n1-element Vector{Float64}:\n 0.5\n\n\n\n\n\n","category":"type"},{"location":"parameterizable_transform/#AdvancedBoosting.MultiTransform","page":"Parameterizable Transformations","title":"AdvancedBoosting.MultiTransform","text":"Applies multiple ParameterizableTransforms on the same input and concatenates their outputs. I.e. let g_1(cdotcdot)g_n(cdotcdot) denote a set of n ParameterizableTransforms. Then, the MultiTransform computes as\n\ng(f(mathbfx)mathbfx)=left(g_1(f(mathbfx)mathbfx)g_n(f(mathbfx)mathbfx)right)^T\n\nwhere the outputs of each individual transform are flattened.\n\nusing AdvancedBoosting\ntransform1 = IdentityTransform([1])\ntransform2 = SigmoidTransform([2])\ntransform = MultiTransform([transform1, transform2])\n\ntransform([0.,0.], zeros(3))\n\n# output\n\n2-element Vector{Float64}:\n 0.0\n 0.5\n\nusing AdvancedBoosting\ntransform1 = IdentityTransform([1])\ntransform2 = SigmoidTransform([2])\ntransform = MultiTransform([transform1, transform2])\n\ntransform([0.,0.])\n\n# output\n\n2-element Vector{Float64}:\n 0.0\n 0.5\n\n\n\n\n\n","category":"type"},{"location":"parameterizable_transform/#AdvancedBoosting.VaryingCoefficientTransform","page":"Parameterizable Transformations","title":"AdvancedBoosting.VaryingCoefficientTransform","text":"Applies the outputs of multiple gradient boosting models as a varying coefficient model. I.e., for an input vector mathbfxinmathbbR^M and M stacked boosting models,\n\nf(cdot)=left(f_1(cdot)f_M(cdot)right)^T\n\nwe have\n\ng(f(mathbfx)mathbfxalpha)=alpha + left(f_(i_1)(mathbfx)f__(i_K)(mathbfx)right)^T mathbfx_(i_1i_K)\n\nwhere alpha denotes the intercept of the varying coefficient model and i_1i_K denote the indices defined in target_dims\n\nusing AdvancedBoosting\ntransform = VaryingCoefficientTransform([1,2])\n\ntransform([1.,2.], [1.,2.])\n\n# output\n\n1-element Vector{Float64}:\n 6.0\n\n\n\n\n\n","category":"type"},{"location":"parameterizable_transform/#AdvancedBoosting.ComposedTransform","page":"Parameterizable Transformations","title":"AdvancedBoosting.ComposedTransform","text":"Composes two ParameterizableTransforms as follows:\n\nLet g1g2 denote two ParameterizableTransforms, mathbfx some input vector and mathbff(mathbfx) the outputs of one or more boosting models, then\n\ng(mathbff(mathbfx)mathbfx)=(g_2circ g_1)(mathbff(mathbfx)mathbfx)=g_2(g_1(mathbff(mathbfx)mathbfx)mathbfx)\n\nExample:\n\nusing AdvancedBoosting\ng1 = VaryingCoefficientTransform([1,2])\ng2 = SoftplusTransform([1])\n\ntransform = g2∘g1\n\ntransform([1.,2.], [1.,2.])\n\n# output\n\n1-element Vector{Float64}:\n 6.00247568513773\n\n\n\n\n\n","category":"type"},{"location":"parameterizable_transform/#AdvancedBoosting.SumTransform","page":"Parameterizable Transformations","title":"AdvancedBoosting.SumTransform","text":"Sums two ParameterizableTransforms as follows:\n\nLet g1g2 denote two ParameterizableTransforms, mathbfx some input vector and mathbff(mathbfx) the outputs of one or more boosting models, then\n\ng(mathbff(mathbfx)mathbfx)=(g_1 + g_2)(mathbff(mathbfx)mathbfx)=g_1(mathbff(mathbfx)mathbfx) + g_2(mathbff(mathbfx)mathbfx)\n\nExample:\n\nusing AdvancedBoosting\ng1 = SoftplusTransform([1])\ng2 = SoftplusTransform([1])\n\ntransform = g2+g1\n\ntransform([1.], [1.])\n\n# output\n\n1-element Vector{Float64}:\n 2.6265233750364456\n\n\n\n\n\n","category":"type"}]
}
